{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Replication paper Attention is all you need\n",
        "\n",
        "https://arxiv.org/pdf/1706.03762.pdf"
      ],
      "metadata": {
        "id": "FMp0G_FPEhQD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWw-zzrXET7v",
        "outputId": "7ba2ba1b-183a-4662-f79b-0882b0421570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 1.13.1+cu116\n",
            "torchvision version: 0.14.1+cu116\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMJZfpNLEzLq",
        "outputId": "4755e585-858a-4ebd-e400-b9095a3b37ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 3435, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 3435 (delta 55), reused 97 (delta 41), pack-reused 3302\u001b[K\n",
            "Receiving objects: 100% (3435/3435), 643.58 MiB | 24.70 MiB/s, done.\n",
            "Resolving deltas: 100% (1962/1962), done.\n",
            "Updating files: 100% (222/222), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "btOFL87LEzhR",
        "outputId": "c51ca95b-df76-4779-a23d-ec13ed4cea91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "G1My9fQTJX9e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10CrwCc8JaTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxaqFSJyJaIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktv2TDXxE1yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SelfAttention"
      ],
      "metadata": {
        "id": "IlJ8zwrHJbHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_size, heads_num):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.heads_num = heads_num\n",
        "    self.head_dim = embedding_size//heads_num\n",
        "\n",
        "    assert (self.head_dim * heads_num == embedding_size), \"Embedding size needs to divisible by head number\"\n",
        "\n",
        "    self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "\n",
        "    self.fully_connected = nn.Linear(heads_num*self.head_dim, embedding_size)\n",
        "\n",
        "  def forward(self, values, keys, query, mask):\n",
        "    N = query.shape[0]\n",
        "    value_len, keys_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "    values = values.reshape(N, value_len, self.heads_num, self.head_dim)\n",
        "    keys = keys.reshape(N, keys_len, self.heads_num, self.head_dim)\n",
        "    queries = query.reshape(N, query_len, self.heads_num, self.head_dim)\n",
        "\n",
        "    values = self.values(values)\n",
        "    keys = self.keys(keys)\n",
        "    queries = self.queries(queries)\n",
        "\n",
        "\n",
        "    energy = torch.einsum(\"nqhd,nkhd->nhqk\",[queries,keys])\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0,  float(\"-1e20\"))\n",
        "    \n",
        "    attention = torch.softmax(energy / (self.embedding_size ** (1/2)), dim=3)\n",
        "\n",
        "    out = torch.einsum(\"nhql,nlhd->nqhd\",[attention, values]).reshape(\n",
        "        N, query_len, self.heads_num*self.head_dim\n",
        "    )\n",
        "\n",
        "    out = self.fully_connected(out)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "ZNHY357BJePC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "    super().__init__()\n",
        "    self.attention = SelfAttention(embedding_size=embed_size, heads_num=heads)\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(forward_expansion*embed_size, embed_size)\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, value, key, query, mask):\n",
        "    attention = self.attention(value, key, query, mask)\n",
        "    x = self.dropout(self.norm1(attention + query))\n",
        "    forward = self.feed_forward(x)\n",
        "    out = self.dropout(self.norm2(forward + x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "ht8tr1f-i4mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               src_vocab_size,\n",
        "               embed_size,\n",
        "               num_layers,\n",
        "               heads,\n",
        "               device,\n",
        "               forward_expansion,\n",
        "               dropout,\n",
        "               max_length):\n",
        "    super().__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.device = device,\n",
        "    self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerBlock(embed_size=embed_size,heads=heads, dropout=dropout, forward_expansion=forward_expansion)\n",
        "            for _ in range(num_layers)\n",
        "        \n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    N, seq_lenght = x.shape\n",
        "    self.device= \"cpu\"\n",
        "    positions = torch.arange(0, seq_lenght).expand(N, seq_lenght).to(self.device)\n",
        "\n",
        "    out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(out,out,out,mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "pTQt8oujkycN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FP1AmZaenXHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embed_size,\n",
        "               heads,\n",
        "               forward_expansion,\n",
        "               dropout,\n",
        "               device):\n",
        "    super().__init__()\n",
        "    self.attention = SelfAttention(embed_size, heads)\n",
        "    self.norm = nn.LayerNorm(embed_size)\n",
        "    self.transformer_block = TransformerBlock(\n",
        "          embed_size, heads, dropout, forward_expansion\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, value, key, src_mask, trg_mask):\n",
        "    attention = self.attention(x,x,x,trg_mask)\n",
        "    query = self.dropout(self.norm(attention + x))\n",
        "    out = self.transformer_block(value, key, query, src_mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "f_qmC-rNnCk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               trg_vocab_size,\n",
        "               embed_size,\n",
        "               num_layers,\n",
        "               heads,\n",
        "               forward_expansion,\n",
        "               dropout,\n",
        "               device,\n",
        "               max_length):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "        [DecoderBlock(embed_size=embed_size,heads=heads, forward_expansion=forward_expansion,dropout=dropout,device=device)\n",
        "        for _ in range(num_layers)]\n",
        "    )\n",
        "\n",
        "    self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "    N, seq_lenght = x.shape\n",
        "    positions = torch.arange(0, seq_lenght).expand(N, seq_lenght).to(self.device)\n",
        "    x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x,enc_out,enc_out, src_mask, trg_mask)\n",
        "\n",
        "    out = self.fc_out(x)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "WCUldJ6wor7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               src_vocab_size,\n",
        "               trg_vocab_size,\n",
        "               src_pad_idx,\n",
        "               trg_pad_idx,\n",
        "               embed_size=256,\n",
        "               num_layers=6,\n",
        "               forward_expansion=4,\n",
        "               heads=8,\n",
        "               dropout=0,\n",
        "               device=\"cpu\",\n",
        "               max_length=100):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        src_vocab_size=src_vocab_size, embed_size=embed_size,\n",
        "         num_layers=num_layers, heads=heads, device=device,\n",
        "         forward_expansion=forward_expansion,\n",
        "        dropout=dropout, max_length=max_length\n",
        "    )\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        trg_vocab_size=trg_vocab_size,\n",
        "        embed_size=embed_size, num_layers=num_layers,\n",
        "        heads=heads,forward_expansion=forward_expansion,\n",
        "        dropout=dropout, device=device, max_length=max_length\n",
        "    )\n",
        "\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask.to(self.device)\n",
        "  \n",
        "  def make_trg_mask(self, trg):\n",
        "    N, trg_len = trg.shape\n",
        "    trg_mask = torch.tril(torch.ones((trg_len,trg_len))).expand(\n",
        "        N, 1, trg_len, trg_len\n",
        "    )\n",
        "    return trg_mask.to(self.device)\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "    enc_src = self.encoder(src,src_mask)\n",
        "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "Ql0iRzICq1L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "\n",
        "x = torch.tensor([[1,5,6,4,3,9,5,2,0],[1,8,7,3,4,5,6,7,2]]).to(device)\n",
        "\n",
        "trg = torch.tensor([[1,7,4,3,5,9,2,0],[1,5,6,2,4,7,6,2]]).to(device)\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "\n",
        "src_vocab_size = 10\n",
        "trg_vocab_size = 10\n",
        "\n",
        "model = Transformer(src_vocab_size,trg_vocab_size,src_pad_idx,trg_vocab_size).to(device)\n",
        "\n",
        "out = model(x, trg[:, :-1])\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxFL8WYftv_1",
        "outputId": "d1be8e12-0a42-4b3f-9407-0a5f5459d2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 7, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#My try"
      ],
      "metadata": {
        "id": "4POBF-Esp1Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multihead Self-Attention"
      ],
      "metadata": {
        "id": "kiAZYgB3VElY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttentionAndNormBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embed_dim:int = 512,\n",
        "               num_heads:int = 6,\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
        "                                                num_heads=num_heads,\n",
        "                                                batch_first=False)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_out, _ = self.multihead_attn(query=x,\n",
        "                                           key=x,\n",
        "                                           value=x,\n",
        "                                           need_weights=False)\n",
        "    \n",
        "    residual_conn = attention_out + x\n",
        "    block_out = self.layer_norm(residual_conn)\n",
        "    return block_out"
      ],
      "metadata": {
        "id": "NQpVcrpXp3c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardAndNorm(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               embed_dim:int = 512,\n",
        "               ffn_size:int = 2048):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(in_features=embed_dim,\n",
        "                  out_features=ffn_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=ffn_size,\n",
        "                  out_features=embed_dim)\n",
        "    )\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    ffn_out = self.ffn(x)\n",
        "    residual_conn = ffn_out + x\n",
        "    block_out = self.layer_norm(residual_conn)\n",
        "    return block_out\n"
      ],
      "metadata": {
        "id": "8UyUEBriaEWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ],
      "metadata": {
        "id": "U3AWnPQvbk7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def _init__(self,\n",
        "              embed_dim:int = 512,\n",
        "              num_heads:int = 6,\n",
        "              ffn_size = 512):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mhsan_block = MultiHeadSelfAttentionAndNormBlock(embed_dim=embed_dim,\n",
        "                                                num_heads=num_heads)\n",
        "    \n",
        "    self.ffn_block = FeedForwardAndNorm(embed_dim = embed_dim,\n",
        "                                        ffn_size = ffn_size)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.mhsan_block(x)\n",
        "    x = self.ffn_block(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "E9impHCubhLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decoder"
      ],
      "metadata": {
        "id": "PlcQNo35erRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init_(self,\n",
        "              encoder_out,\n",
        "              embed_dim = 512,\n",
        "              num_heads = 6,\n",
        "              ffn_size = 512\n",
        "              ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mhsan_block1 = MultiHeadSelfAttentionAndNormBlock(embed_dim=embed_dim,\n",
        "                                                num_heads=num_heads)\n",
        "    self.mhsan_block2 = MultiHeadSelfAttentionAndNormBlock(embed_dim=embed_dim,\n",
        "                                                num_heads=num_heads)\n",
        "    \n",
        "    self.ffn_block = FeedForwardAndNorm(embed_dim = embed_dim,\n",
        "                                        ffn_size = ffn_size)\n",
        "    \n",
        "  def forward(self, x):    \n",
        "    x = self.mhsan_block(x)\n",
        "    x = self.mhsan_block(x)\n",
        "    x = self.ffn_block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ExenY-ajc8JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer"
      ],
      "metadata": {
        "id": "mvfNpXg_gbSp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UecmzaaRgZFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}