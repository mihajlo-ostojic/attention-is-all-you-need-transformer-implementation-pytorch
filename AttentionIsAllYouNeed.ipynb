{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WsOTf9WvaGfw",
        "9bwUSaMAuuEV",
        "U3AWnPQvbk7-",
        "PlcQNo35erRp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Replication of paper Attention is all you need\n",
        "\n",
        "https://arxiv.org/pdf/1706.03762.pdf"
      ],
      "metadata": {
        "id": "FMp0G_FPEhQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0.Imports"
      ],
      "metadata": {
        "id": "WsOTf9WvaGfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWw-zzrXET7v",
        "outputId": "8979fe9f-0631-4f72-f502-6214a35cf239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "torch version: 2.0.0+cu118\n",
            "torchvision version: 0.15.1+cu118\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMJZfpNLEzLq",
        "outputId": "fd8bb025-8be0-4ffe-fa10-9fd0c0c14740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 3578, done.\u001b[K\n",
            "remote: Counting objects: 100% (200/200), done.\u001b[K\n",
            "remote: Compressing objects: 100% (113/113), done.\u001b[K\n",
            "remote: Total 3578 (delta 88), reused 189 (delta 82), pack-reused 3378\u001b[K\n",
            "Receiving objects: 100% (3578/3578), 647.31 MiB | 25.98 MiB/s, done.\n",
            "Resolving deltas: 100% (2044/2044), done.\n",
            "Updating files: 100% (240/240), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btOFL87LEzhR",
        "outputId": "1796ad2e-7b5a-48e4-9dc0-ff3f92e7cf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Model Definition"
      ],
      "metadata": {
        "id": "G1My9fQTJX9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test IGNORE THIS\n",
        "# import torch\n",
        "\n",
        "# # Create a 2D tensor of size 10x20 filled with zeros\n",
        "# tensor = torch.zeros(6, 6)\n",
        "\n",
        "# # Set the values above the main diagonal to 0.0001\n",
        "# tensor = tensor + 0.0001 * torch.triu(torch.ones(6, 6), diagonal=1)\n",
        "\n",
        "# # Print the tensor\n",
        "# tensor"
      ],
      "metadata": {
        "id": "wwBsZa18XZbe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Defining basic blocks of model\n",
        "\n",
        "\n",
        "From paper page number 3, paragraph 1:\n",
        "\n",
        "\"...The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization..\"\n",
        "\n",
        "From pager page number 7, last paragraph:\n",
        "\n",
        "\"We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "yMVPuywzlg2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1.1 Multihead Self-Attention Sublayer with Normalization"
      ],
      "metadata": {
        "id": "kiAZYgB3VElY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttentionAndNormBlock(nn.Module):\n",
        "  \n",
        "  def __init__(self,\n",
        "               embed_dim:int = 512,\n",
        "               num_heads:int = 6,\n",
        "               masked:bool = False,\n",
        "               dropout:float = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
        "                                                num_heads=num_heads,\n",
        "                                                batch_first=False)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.masked = masked\n",
        "\n",
        "  def forward(self, x, query, key, value):\n",
        "    \n",
        "    \n",
        "    if masked:\n",
        "      mask = torch.zeros(self.embed_dim, self.embed_dim)\n",
        "      mask = mask - 0.0000000001 * torch.triu(torch.ones(self.embed_dim, self.embed_dim), diagonal=1)\n",
        "      \n",
        "      attention_out, _ = self.multihead_attn(query=query,\n",
        "                                           key=key,\n",
        "                                           value=value,\n",
        "                                           attn_mask=mask,\n",
        "                                           need_weights=False)\n",
        "    else:\n",
        "      attention_out, _ = self.multihead_attn(query=query,\n",
        "                                           key=key,\n",
        "                                           value=value,\n",
        "                                           need_weights=False)\n",
        "    dp = self.dropout(attention_out)\n",
        "    residual_conn = dp + x\n",
        "    block_out = self.layer_norm(residual_conn)\n",
        "    return block_out"
      ],
      "metadata": {
        "id": "NQpVcrpXp3c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1.2 FeedForward Sublayer with Normalization"
      ],
      "metadata": {
        "id": "9bwUSaMAuuEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardAndNorm(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               embed_dim:int = 512,\n",
        "               ffn_size:int = 2048,\n",
        "               dropout:float = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(in_features=embed_dim,\n",
        "                  out_features=ffn_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=ffn_size,\n",
        "                  out_features=embed_dim)\n",
        "    )\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    ffn_out = self.ffn(x)\n",
        "    dp = self.dropout(ffn_out)\n",
        "    residual_conn = dp + x\n",
        "    block_out = self.layer_norm(residual_conn)\n",
        "    return block_out\n"
      ],
      "metadata": {
        "id": "8UyUEBriaEWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Encoder Block"
      ],
      "metadata": {
        "id": "U3AWnPQvbk7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From paper page number 3, paragraph 1: \n",
        "\n",
        "\"The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. ...\"\n",
        "\n",
        "\n",
        "\n",
        "Also check out Figure 1: The Transformer - model architecture, for better underestanding of arhitecture "
      ],
      "metadata": {
        "id": "QIK6_a2WiBXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def _init__(self,\n",
        "              embed_dim:int = 512,\n",
        "              num_heads:int = 6,\n",
        "              ffn_size = 2048):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mhsan_block = MultiHeadSelfAttentionAndNormBlock(embed_dim=embed_dim,\n",
        "                                                num_heads=num_heads)\n",
        "    \n",
        "    self.ffn_block = FeedForwardAndNorm(embed_dim = embed_dim,\n",
        "                                        ffn_size = ffn_size)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.mhsan_block(x,x,x,x)\n",
        "    x = self.ffn_block(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "E9impHCubhLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Decoder Block"
      ],
      "metadata": {
        "id": "PlcQNo35erRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From paper page number 3, paragraph 1:\n",
        "\n",
        "\n",
        "\"The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. ...\" \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tY7eFRo6iolY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init_(self,\n",
        "              embed_dim = 512,\n",
        "              num_heads = 6,\n",
        "              ffn_size = 2048\n",
        "              ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mhsan_block1 = MultiHeadSelfAttentionAndNormBlock(embed_dim=embed_dim,\n",
        "                                                num_heads=num_heads,masked=True)\n",
        "\n",
        "    self.mhsan_block2 = MultiHeadSelfAttentionAndNormBlock(embed_dim=embed_dim,\n",
        "                                                num_heads=num_heads)\n",
        "    \n",
        "    self.ffn_block = FeedForwardAndNorm(embed_dim = embed_dim,\n",
        "                                        ffn_size = ffn_size)\n",
        "    \n",
        "  def forward(self, x, encoder_out):    \n",
        "    block1_out = self.mhsan_block(x,x,x,x)\n",
        "    block2_out = self.mhsan_block(block1_out,encoder_out,encoder_out,block1_out)\n",
        "    out = self.ffn_block(block2_out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "ExenY-ajc8JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4 Transformer Model"
      ],
      "metadata": {
        "id": "mvfNpXg_gbSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init_(self,\n",
        "              embed_dim = 512,\n",
        "              num_heads = 6,\n",
        "              ffn_size = 2048,\n",
        "              enc_layer_num = 6,\n",
        "              dec_layer_num = 6\n",
        "              ):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.ffn_size = ffn_size\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList(\n",
        "        [\n",
        "            EncoderBlock(embed_dim=embed_dim, num_heads=num_heads, ffn_size=ffn_size)\n",
        "            for _ in range(enc_layer_num)\n",
        "        \n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.decoder_layers = nn.ModuleList(\n",
        "        [\n",
        "            DecoderBlock(embed_dim=embed_dim, num_heads=num_heads, ffn_size=ffn_size)\n",
        "            for _ in range(dec_layer_num)\n",
        "        \n",
        "        ]\n",
        "    )\n",
        "  \n",
        "  def forward(self, encoder_input, decoder_input):    \n",
        "    \n",
        "    x = encoder_input\n",
        "    for layer in self.encoder_layers:\n",
        "      x = layer(x)\n",
        "    \n",
        "    \n",
        "    encoder_out = x\n",
        "\n",
        "    x = decoder_input\n",
        "    for layer in self.decoder_layers:\n",
        "      x = layer(x, encoder_out)\n",
        "    \n",
        "    decoder_out = x\n",
        "\n",
        "\n",
        "    #TODO: add linear and softmax layer\n",
        "\n",
        "    return decoder_out"
      ],
      "metadata": {
        "id": "UecmzaaRgZFq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "0c69fb84-e58f-4805-e355-02c79314fbbb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bb4d4bfb97b8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTransformerBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   def __init_(self,\n\u001b[1;32m      3\u001b[0m               \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mffn_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dUbRNfv-hKvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Positional embedings"
      ],
      "metadata": {
        "id": "AcJxDhBUgpUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "#position_embedding = nn.Embedding(max_length, embed_size)"
      ],
      "metadata": {
        "id": "dE1K6koYgsyY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}